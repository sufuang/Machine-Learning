{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  User base recommendation for store ID 0025 from 06-01-2019 to 08-31-2019  \n",
    "Purpose: \n",
    "1. Using SQL Assistant to create a table temp_tables.rs_pos_tnx_s0025_junaug19_sum by extracting/cleaning/aggregating data from Teradata view. Please refer 'Input' below.  \n",
    "2. Apply Scikit-Surprise to pick up top 10 recommended items for every user.\n",
    "\n",
    "## Introduction\n",
    "### Scikit-Surprise -  Simple Python Recommendation System Engine\n",
    "- Scikit-Surprise is an easy-to-use Python Scikit for recommender systems.\n",
    "- The surprise only accepts the dataframe with user_id, item_id and rating.\n",
    "- The input to our recommendation engine is a table with columns of hh_sk, prod_sk, and unit_qty\n",
    "  - hh_sk is equivalent to user_id\n",
    "  - prod_sk is equivalent to item_id\n",
    "  - unit_qty is equivalent to rating \n",
    "- Most estimators in Scikit can be used in surprise\n",
    "  - Would apply KNN algorithms with variety of similarity measures to get the similarity matrix  \n",
    "  - KNN algorithms:\n",
    "    - KNNBasic: A basic collaborative filtering algorithm\n",
    "    - KNNWithMeans: A basic collaborative filtering algorithm, taking into account the mean ratings of each user\n",
    "    - KNNWithZScore: A basic collaborative filtering algorithm, taking into account the z-score normalization of each user\n",
    "    - KNNBaseline: A basic collaborative filtering algorithm taking into account a baseline rating\n",
    "  - Available similarity measures:\n",
    "    - cosine : Compute the cosine similarity between all pairs of users (or items)\n",
    "    - msd\t : Compute the Mean Squared Difference similarity between all pairs of users (or items)\n",
    "    - pearson: Compute the Pearson correlation coefficient between all pairs of users (or items)\n",
    "    - pearson_baseline: Compute the (shrunk) Pearson correlation coefficient between all pairs of users (or items) using baselines for centering instead of means    \n",
    "  - Similarity matrix\n",
    "     - User-based similarity \n",
    "     - Content-based similarity  \n",
    " - Will use hit-rate to evaluate the recommendation system.\n",
    " \n",
    "### Collaborative filtering\n",
    "   - Will apply Neighborhood-Based Collaborative Filtering to build the Recommendation Engine. \n",
    "     - Find other people like you and recommend stuff they like. \n",
    "   - In Collaborative Filtering, the model learns from the user’s past behavior, user’s decision, preference to     predict items the user might have an interest in.\n",
    "   - Can apply user-based/item_based collaborative filtering\n",
    "\n",
    "## This module includes the following steps:\n",
    "   \n",
    "1. Select most popular user (hh_sk) and products (prod_sk) and create a dataset required by surprise\n",
    "2. Apply KNN algorithm to get user similarity matrix\n",
    "3. Apply LeaveOneOut to get the trainSet and test dataset  \n",
    "4. Based on the user similarity matrix to get top K similar user  \n",
    "5. Based on the similarity score and unit_qty to get uqtysum\n",
    "6. Use the uqtysum to get top N recommended items\n",
    "7. Calculate the hit rate\n",
    "8. Save the top K similar user/neighbors and top N recommended items in a Teradata table\n",
    "\n",
    "## Input\n",
    "  1. Create temp_tables.rs_pos_tnx_s0025_junaug19 with column hh_sk, prod_sk, unit_qty\n",
    "     - Input: Teradata view: dw_bi_vw.F_POS_TXN_DTL\n",
    "     - Selection criteria:         ; only select\n",
    "       - STR_FAC_NBR  = 0025 (store ID) \n",
    "       - txn_dt between  '2019-06-01' and  '2019-08-31' \n",
    "       - hh_sk > 0 and  prod_sk > 0 and unit_qty betwen 1 and 10 \n",
    "       - wgt_prod_ind = 0 (Purchased the product by unit) \n",
    "     -  SQL to create the table \n",
    "        \"create table temp_tables.rs_pos_tnx_s0025_junaug19 as ( sel txn_dt, hh_sk, prod_sk, unit_qty  \n",
    "           from dw_bi_vw.F_POS_TXN_DTL where STR_FAC_NBR  = 0025 and WGT_PROD_IND = 0 \n",
    "            and prod_sk > 0 and hh_sk > 0 \n",
    "            and unit_qty between 1 and 10 \n",
    "            and txn_dt    between '2019-06-01' and  '2019-08-31' ) with data\"\"\" \n",
    "   2. Create temp_tables.rs_pos_tnx_s0025_junaug19_sum\n",
    "      - Aggregate unit_qty by grouping hh_sk and prod_sk\n",
    "      - There are 15306 with count(HH_SK) > 9  and 3972 with (PROD_SK) > 49\n",
    "        - The number will be used to select most popular user (hh_sk) and products (prod_sk)   \n",
    "        \n",
    "## Output \n",
    "     - Top N recommended items\n",
    "     - A dataframe with top N recommended items and top K similar users/neighbors\n",
    "     \n",
    "## Module name: rs_user_base.ipynb\n",
    " Author     : Sophia Yue \n",
    " Date       : Sep 2019\n",
    "\n",
    "## Comments\n",
    "    - Use hit_rate to validate the result\n",
    "    - There are varities of KNN algorithms to get the similarity matrix  Mcombinations to get the top N recommendation\n",
    "      - The Person Baseline, KNNBaseline, would get the hightest hit rate  \n",
    "    - The hit rate would not improve too much by using mean from unit_qty for the nomalization \n",
    "      - The hit rate = 0.0426\n",
    "    -  The hit rate by using max score = 10 for the nomalization  is 0.0434\n",
    "    - The KNNBasic and mean from unit_qty for the nomalization are used for the recommendation engine  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#1\n",
    "def cf_get_userbase_topN(trainSet, simsMatrix, K, N, nrm_adj, verbose = True):\n",
    "   \"\"\"\n",
    "    Module name: cf_get_userbase_ntopN\n",
    "    Purpose    : Function to get user base top N recommended items  \n",
    "    Parameters:\n",
    "       trainSet   : Surprise training set, created from LeaveOneOut \n",
    "                    with columns of \n",
    "                    - ihh_sk is internal id of hh_sk\n",
    "                      - Surprise will convert the hh_sk to a range of 0 to no of hh_sk - 1\n",
    "                    - iprod_sk is internal id of prod_sk\n",
    "                      - Surprise will convert the hh_sk to a range of 0 to no of prod_sk - 1\n",
    "                    - unit_qty is equivalent to rating  \n",
    "       simsMatrix : Similarity Matrix created by KNN algorithm  \n",
    "       K          : No of nearest users    \n",
    "       N          : Total number of products to be recommended \n",
    "       nrm_adj    : A dictionary with key as iprod_sk and value as mean of unit_qty or max scale\n",
    "                    - Will dynamically generate code to calculate adjusted unit_qty  \n",
    "       verbose    : Weather to print the message of elapse time of executing the function\n",
    "                     Default is Truer\n",
    "     Return\n",
    "       topN       : Default dictionary to keep N recommended items for each user \n",
    "       knbr_ary   : Array of k nearest neighbor raw_id and scores which will be combined with topN to create a dataframe\n",
    " \n",
    "  Functions: A loop to go through all the user in trainSet to build top N recommendation\n",
    "    0. Dynamically generate code to calculate adjusted unit_qty  \n",
    "    1. Build a list of similar user from simsMatrix with (innerID, score)   \n",
    "    2. Get top 10 similar user 'kNeighbors' by using Python method heapq.nlargest to sort similarUsers\n",
    "      - kNeighbors is a list of tuple with (innerID, score) \n",
    "        - innerID: inner user id\n",
    "    3. Invoke c_cr_nbrK_list to create a list of raw iuid, raw iid and score in kNeighbors     \n",
    "    4. Build a defaultdict 'candidates' with key = itemID and value = recalculated rate (ratingSum)\n",
    "       - For each user in kNeighbors, recalculating rate for each items  \n",
    "        - Apply 'trainSet.ur' to get all the items and rating \n",
    "        - Add up ratings for each item, weighted by user similarity     \n",
    "    5.  Build a dictionary with itemId that the user has already bought\n",
    "        - key: itemId \n",
    "        - value : 1 \n",
    "    6. Get top N  recommended items \n",
    "   \"\"\" \n",
    "   fnc_name = inspect.stack()[0][3]\n",
    "   start_time = time.time()\n",
    "   topN = defaultdict(list)\n",
    "   nbrK_ary = []  # array for k nighbor raw_id and scores which will be combine with topN to create a dataframe\n",
    "   \n",
    "   \"\"\"\n",
    "     Perform loop to get top N similar users for each iuid defined in trainSet\n",
    "     - n_users: Total number of users\n",
    "     - iuid: Internal user Id\n",
    "        - iuid between 0 and (n_users - 1)  \n",
    "   \n",
    "   \"\"\"  \n",
    "   for iuid in range(trainSet.n_users): \n",
    "         \"\"\"\n",
    "           1. Build a list of similar users and score from simsMatrix\n",
    "              - Use inner user id, iuid, from TrainSet to get similarityRow from simsMatrix\n",
    "              - Use simsMatrix to create a list of similarUsers\n",
    "         \"\"\"\n",
    "         similarityRow = simsMatrix[iuid]       \n",
    "         similarUsers = []        \n",
    "         for innerID, score in enumerate(similarityRow):\n",
    "             if (innerID != iuid):\n",
    "                 similarUsers.append( (innerID, score) )             \n",
    "            \n",
    "         \"\"\"\n",
    "            2. Get top 10 similar user 'kNeighbors' by using Python method heapq.nlargest to sort similarUsers\n",
    "               - innerID: inner user id\n",
    "               - kNeighbors is a list of tuple with (innerID, score), e.g.  \n",
    "                 [(iid0, score0), (iid1, score1) .... (iid9, score9)] \n",
    "                  - iid0 would have the highest score and is close to iuid most\n",
    "               -  similarUsers is a list of tuple of inner ID and score\n",
    "                  - Inside 'heapq.nlargest', name the tuple as simRow\n",
    "                     - simRow[1] will be the score\n",
    "                        - Use score to sore similarUsers and get top 10 scores\n",
    "         \"\"\"\n",
    "         kNeighbors = heapq.nlargest(K, similarUsers, key=lambda simRow: simRow[1]) \n",
    "           \n",
    "         \"\"\"\n",
    "            3. Invoke c_cr_nbrK_list to create a list of raw iuid, all the raw iid and score in kNeighbors \n",
    "         \"\"\"   \n",
    "         nbrK = cf_cr_nbrK_list(iuid, kNeighbors)         \n",
    "         nbrK_ary.append(nbrK)\n",
    "            \n",
    "         \"\"\"\n",
    "            4. Build a defaultdict 'candidates' with key = itemID and value = recaculated unit_qty (uqtySum)\n",
    "               - Get the sum of unit_qty, and add up them for each candidate, weighted by user similarity score\n",
    "                 - For each user in kNeighbors, recalculating unit_qty for each items  \n",
    "                 - Apply 'trainSet.ur' to get all the iprod_sk and unit_qty \n",
    "                 - Add up nomalized unit_qty for each iprod_sk, weighted by user similarity\n",
    "                   - Utilize mean of unit_qty for iprod_sk for the nomalization \n",
    "                - theirUqtys: A list of iprod_sk, unit qty for all items been purchased by candidates, e.g\n",
    "                  [(385, 4),(143, 3), (99,1), (179,5 ...)] \n",
    "                   - 385, 143, 99 ... are innerID\n",
    "                   - 4, 3, 1 ... are sum of adjusted unit_qty\n",
    "                         \n",
    "         \"\"\"     \n",
    "         candidates = defaultdict(float)\n",
    "         for similarUser in kNeighbors:\n",
    "           innerID = similarUser[0]\n",
    "           userSimilarityScore = similarUser[1]\n",
    "\n",
    "           theirUqtys = trainSet.ur[innerID]\n",
    "           for iprod_sk, unit_qty in theirUqtys:\n",
    "               if type(nrm_adj) is dict:\n",
    "                  u_mean = nrm_adj.get(iprod_sk) \n",
    "                  candidates[iprod_sk] += (unit_qty / u_mean) * userSimilarityScore \n",
    "               else:                       \n",
    "                  candidates[iprod_sk] += (unit_qty / nrm_adj) * userSimilarityScore  \n",
    "         \"\"\" \n",
    "           5.  Build a dictionary with iprod_sk that the user has already bought\n",
    "               - key: iprod_sk \n",
    "               - value : 1\n",
    "         \"\"\"\n",
    "         d_bought = {}\n",
    "         for _iprod_sk, _unit_qty in trainSet.ur[iuid]:\n",
    "             d_bought[_iprod_sk] = 1\n",
    "                \n",
    "         \"\"\"    \n",
    "          6. Get top N prod_sk and unit_qty from similar users\n",
    "             - Sort uqtySum from candidate to get top 10 prod_sk\n",
    "             - Type of topN is defaultdict(list)              \n",
    "               - A list of dictionary with hh_sk as key  and a list of top 10 prod_sk, and uqtySum as value, e.g.\n",
    "                 {230: [[173497, 0.9], [1402158, 0.9], [562847, 0.8], [216746, 0.8], [1376094, 0.8], [1354393, 0.8],\n",
    "                       [361557, 0.6], [2049747, 0.5], [2045235, 0.5], [1107573, 0.5], [129589, 0.4]],\n",
    "                  470: [[2001034, 1.0], [990262, 0.8], [1815395, 0.8], [1001413, 0.6], [2225074, 0.6], [1354393, 0.6],\n",
    "                       [669033, 0.5], [651610, 0.5], [727270, 0.5], [159076, 0.5], [1578298, 0.5]],\n",
    "                  ....}\n",
    "         \"\"\"\n",
    "         pos = 1\n",
    "         for _iprod_sk, uqtySum in sorted(candidates.items(), key=itemgetter(1), reverse=True):\n",
    "             if not _iprod_sk in d_bought:\n",
    "                 prodID = trainSet.to_raw_iid(_iprod_sk)\n",
    "                 #topN[int(trainSet.to_raw_uid(iuid))].append( (int(prodID), ratingSum) )\n",
    "                 topN[int(trainSet.to_raw_uid(iuid))].append( [int(prodID), round(uqtySum, 4)] ) \n",
    "                 pos += 1\n",
    "                 if (pos > N):\n",
    "                     break\n",
    "   end_time = time.time()\n",
    "   cf_elapse_time (  start_time, end_time, \"Function {0} completed.\".format(fnc_name))\n",
    "   return topN, nbrK_ary\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    " def cf_cr_nbrK_list(uiid, kNeighbors):\n",
    "       \"\"\"\n",
    "         module name: cf_cr_nbrK_list\n",
    "         Purpose    : Create a list of raw uiid, raw iid and score in kNeighbors\n",
    "                      - The list will create an array and will be used to merge with topN items  \n",
    "         Parameter:\n",
    "           uiid: Internal user id\n",
    "           kNeighbors: A list of  tuple with iid and score for k Neighbors for uiid      \n",
    "        return:\n",
    "           A list with raw uiid, raw iid and score in kNeighbors  \n",
    "       \"\"\" \n",
    "    \n",
    "       \"\"\"\n",
    "         bnbr_x is by converting/flatening the list of tuple in kNeighbors to a flat list, e.g.\n",
    "         [(uiid0, score0), (uiid1, score1) .... (uiid9, score9)] -> [uiid0, score0, uiid1, score1 .... uiid9, score9] \n",
    "       \"\"\"\n",
    "       knbr_x =  list (itertools.chain(*kNeighbors)) \n",
    "        \n",
    "       \"\"\"\n",
    "        knbr_y is a list of tuple of raw id  and score, e.g \n",
    "        [uiid0, score0, uiid1, score1 .... uiid9, score9]  -> [('rid0', score0), ('rid1', score1) .... ('rid9', score9)] \n",
    "       \"\"\"\n",
    "       knbr_id     =  knbr_x[::2]      # Get internal user id from knbr_x (:2 would skip the score)\n",
    "       knbr_rnkscr = knbr_x[1::2]      # get rank for itembase or score for userbase from knbr_x \n",
    "       knbr_rawid  = [trainSet.to_raw_uid(x) for x in knbr_id]  # Convert internal user id to raw id  \n",
    "       knbr_y = list(zip(knbr_rawid, knbr_rnkscr) )   \n",
    "    \n",
    "       \"\"\"\n",
    "       knbr is a list of target user raw id, and raw id and score of user which  is close to the target user, e.g, \n",
    "       ['rid', 'rid0', score0,  .... 'rid9', score9]\n",
    "       \"\"\"\n",
    "       return  [trainSet.to_raw_uid(uiid)] + list (itertools.chain(*knbr_y)) # convert list of tuple to a list\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3\n",
    "def cf_get_unit_qty_mean(s_dataset):\n",
    "   \"\"\"\n",
    "     module name: c_get_unit_qty_mean\n",
    "     Purpose    : Common function to reate a dictionay with key as iprod_sk and value as mean of unit_qty                    \n",
    "     Parameter\n",
    "       s_dataset : Surprise  dataset with columns of ihh_sk, iprod_sk, unit_qty \n",
    "     \n",
    "    return:\n",
    "       d_mean : a dictionary with key as iprod_sk and value as mean of unit_qty  \n",
    "       df_mean: a dataframe with columns of iprod_sk, prod_sk, and unit_qty_mean \n",
    "   \n",
    "    Steps:\n",
    "        1: Create a dataframe from a list of iprod_sk, prod_sk, unit_qty mean\n",
    "        2: Craete a dataframe to get mean of unit_qty for iprod_sk \n",
    "        3: Build a dictionary with key as iprod_sk and value as mean of unit_qty\n",
    "        4: Build a dataframe as iprod_sk, prod_sk, qty_unit_mean\n",
    "   \"\"\" \n",
    "\n",
    "   fnc_name = inspect.stack()[0][3]\n",
    "   start_time = time.time()\n",
    "\n",
    "   iprod_sk_unit = []\n",
    "   for ihh_sk in range(s_dataset.n_users):\n",
    "       _unit_qty = s_dataset.ur[ihh_sk]\n",
    "       for iprod_sk, unit_qty in _unit_qty:\n",
    "           iprod_sk_unit.append([iprod_sk, int(s_dataset.to_raw_iid(iprod_sk)),unit_qty])\n",
    "   df = pd.DataFrame(iprod_sk_unit)\n",
    "   df.columns = ['iprod_sk', 'prod_sk', 'unit_qty']\n",
    "   print(\"c_get_qty_mean - df.head\", df.head() )\n",
    "   \n",
    "   \"\"\"\n",
    "    step2: Craete a dataframe to get mean of unit_qty group by iprod_sk and prod_sk   \n",
    "           -  df_qty_mean_x with index as iprod_sk and prod_sk with column of mean of unit_qty\n",
    "   \"\"\"\n",
    "   df_qty_mean_x = df.groupby (by = ['iprod_sk', 'prod_sk']).agg({'unit_qty':['mean']}) \n",
    "   \n",
    "   \"\"\"\n",
    "    step3: Build and return a dictionary with key as iprod_sk and value as mean \n",
    "   \"\"\"\n",
    "NoneTypeNoneType\n",
    "   #3: Build a dictionary with key as iprod_sk and value as mean of unit_qty \n",
    "   d_mean = dict(zip(l_iprod_sk, l_mean))\n",
    "    \n",
    "   # Step4: Build a dataframe as iprod_sk, prod_sk, qty_unit_mean\n",
    "   df_mean = pd.DataFrame(mean_ary)\n",
    "   df_mean.columns = ['iprod_sk', 'prod_sk', 'unit_qty_mean']\n",
    "\n",
    "   end_time = time.time()\n",
    "   cf_elapse_time (start_time, end_time, \"Function {0} completed.\".format(fnc_name))\n",
    "   return d_mean, df_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4\n",
    "def cf_build_trainset(tnx_tbl, N, M, maxScale, session):\n",
    "    \"\"\"\n",
    "     module name: cf_build_trainset\n",
    "     Purpose    : build trainset\n",
    "     Parameter\n",
    "       session : session to connect to database\n",
    "       tnx_tbl : Transaction table with schema \n",
    "       N       : No of most popular user\n",
    "       M       : No of most popular product \n",
    "       maxScale: Maximum unit_qty  \n",
    "    return:\n",
    "       trainSet: Training set \n",
    "       testSet : Test set\n",
    "       d_mean: a dictionary with key as iprod_sk and value as mean of unit_qty  ????\n",
    "    Functions:\n",
    "     1. Run a query to read data from Teradata table and create a dataframe\n",
    "     2. Get most popular hh_sk  and prod_sk\n",
    "     3: Build surprise dataset \n",
    "     4: Build trainSet and testSet\n",
    "    Notes:\n",
    "     1. The step of initialization will set up the session to connect to Teradata\n",
    "    \"\"\" \n",
    "    print( \"tnx_tbl\", tnx_tbl,\"N = \",  N, \"m =\", M, maxScale, maxScale )    \n",
    "    print(\"session\", session)\n",
    "    \n",
    "    #1: Run a query to read data from Teradata table and create a dataframe\n",
    "    query = \"\"\"\n",
    "    select HH_SK, PROD_SK, UNIT_QTY   \n",
    "    from {0}\"\"\".format(tnx_tbl)\n",
    "    df_sum_qty = pd.read_sql(query,session) \n",
    "    \n",
    "    #2: Get most popular hh_sk  and prod_sk\n",
    "    user_ids_count = Counter(df_sum_qty.HH_SK)   # type: collections.Counter\n",
    "    prod_ids_count = Counter(df_sum_qty.PROD_SK)\n",
    "   \n",
    "    user_ids = [u for u, c in user_ids_count.most_common(N)]\n",
    "    prod_ids = [m for m, c in prod_ids_count.most_common(M)]\n",
    "    df_sum_qty_final = df_sum_qty[df_sum_qty.HH_SK.isin(user_ids) & df_sum_qty.PROD_SK.isin(prod_ids)]  \n",
    "    \n",
    "    #3: Build surprise dataset                         \n",
    "    reader = Reader(rating_scale=(1, maxScale))  # Reader object; rating_scale is required \n",
    "    data = Dataset.load_from_df(df_sum_qty_final[['HH_SK', 'PROD_SK', 'UNIT_QTY']], reader) # type:  surprise.dataset.DatasetAutoFolds\n",
    "    ft = data.build_full_trainset()\n",
    "    print(\"The total no of users in ft = data.build_full_trainset():\", ft.n_users,  \"The total no of items in ft:\", ft.n_items )\n",
    "    \n",
    "    \"\"\"\n",
    "     4: Build trainSet and testSet\n",
    "     - Set aside one purchase per user for testing\n",
    "       - Randomly remove one row from data to create the testSet and the rest would be trainSet\n",
    "         - If the user only has one row, it'll not in trainSet \n",
    "    \"\"\"\n",
    "    LOOCV = LeaveOneOut(n_splits=1, random_state=1)\n",
    "    for train, test in LOOCV.split(data):\n",
    "        trainSet = train\n",
    "        testSet  =  test\n",
    "    print(\"The total number of users in trainSet:\", trainSet.n_users,  \"The total number of items in trainSet:\", trainSet.n_items ) \n",
    "    print(\"The total length of testSet:\", len(testSet),\"\\nExample of testSet:\", testSet[0:2])     \n",
    "    return trainSet, testSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cf_comb_topN_nbrK(topN, nbrK_ary, lbl_topN, lbl_nbrK ):\n",
    "    \"\"\"\n",
    "     module name: c_comb_topN_nbrK\n",
    "     Purpose    : Buile a dataframe by merging recommendted items and top K neighbors\n",
    "                  - The dataframe will be used to build a table\n",
    "     Parameter:\n",
    "       topN    : Default dictionary to keep N recommended items for each user \n",
    "       nbrK_ary: Array of raw_id and scores for top k neighbor  \n",
    "       lbl_topN: Label for top N prods \n",
    "       lbl_nbrK: Label for top K neighbors  \n",
    "     Return:\n",
    "       df_topN_nbrK: A dataframe by merging recommendted items and top K neighbors   \n",
    "     Functions:\n",
    "      1. Flatten topN to a list\n",
    "        - type of topN is defaultdict(list)\n",
    "          - A list of dictionary with hh_sk as key  and a list of top N prod_sk, and uqtySum as value.\n",
    "      2. Merge recommendted items and similar k neighbor\n",
    "         - user base: similar k neighbors will be similar users  \n",
    "         - item base: similar k neighbors will be similar prods/items \n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "      1.Flatten topN to a list\n",
    "        - Type of topN is defaultdict(list)              \n",
    "          - A list of dictionary with hh_sk as key  and a list of top 10 prod_sk, and uqtySum as value, e.g.\n",
    "            {230: [[173497, 0.9], [1402158, 0.9], [562847, 0.8], [216746, 0.8], [1376094, 0.8], [1354393, 0.8],\n",
    "                  [361557, 0.6], [2049747, 0.5], [2045235, 0.5], [1107573, 0.5], [129589, 0.4]],\n",
    "             470: [[2001034, 1.0], [990262, 0.8], [1815395, 0.8], [1001413, 0.6], [2225074, 0.6], [1354393, 0.6],\n",
    "                  [669033, 0.5], [651610, 0.5], [727270, 0.5], [159076, 0.5], [1578298, 0.5]],\n",
    "             ....}      \n",
    "    \"\"\"    \n",
    "    user_prod_ary = []\n",
    "    for hh_sk, prods in topN.items():\n",
    "         user_prod = [hh_sk] \n",
    "         for j  in range(len(prods)):\n",
    "             user_prod.extend(prods[j])   \n",
    "         user_prod_ary.append(user_prod)\n",
    "    print(user_prod_ary[0:1])\n",
    "    \"\"\" \n",
    "      2. Merge recommendted items and K neighbor\n",
    "         - Create a dataframe for topN \n",
    "         - Create a dataframe for nbrK_ary \n",
    "    \"\"\"\n",
    "    df_topN   = pd.DataFrame.from_records(user_prod_ary, columns = lbl_topN)\n",
    "    df_nbrK   = pd.DataFrame.from_records(nbrK_ary, columns = lbl_nbrK)\n",
    "    df_topN_nbrK = pd.merge(df_topN, df_nbrK, how = 'left', on = ['hh_sk', 'hh_sk'])    \n",
    "    return df_topN_nbrK  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6\n",
    "def cf_get_simsMatrix(s_dataset, sim_measure, user_base_ind, knn_algm ):\n",
    "    \"\"\"\n",
    "      Module name: cf_get_simsMatrix\n",
    "      Purpose    : Function to get similarity matrix \n",
    "      Parameters :\n",
    "       s_dataset : Surprise dataset\n",
    "       sim_measure: Measure for similarity \n",
    "         - Validate values are \"cosine\", \"msd\", \"person\", \"person_baseline\",        \n",
    "       user_base_ind\n",
    "         True  - Apply user_base\n",
    "         False - Apply user_base \n",
    "       Return:\n",
    "        simsMatrix: Simality Matrix\n",
    "    \"\"\"  \n",
    "    fnc_name = inspect.stack()[0][3]\n",
    "    start_time = time.time()    \n",
    "    sim_opt = {'name': sim_measure,\n",
    "                   'user_based': user_base_ind\n",
    "                   }\n",
    "    #v_knn_mdl = \"{0}(sim_options=sim_opt,  verbose = False)\".format(knn_algm)\n",
    "    model = knn_algm(sim_options=sim_opt,  verbose = False)\n",
    "    model.fit(s_dataset)\n",
    "    simsMatrix = model.compute_similarities()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    cf_elapse_time (  start_time, end_time, \"Function {0} completed.\".format(fnc_name))    \n",
    "    return simsMatrix, model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#7\n",
    "def cf_setup_dbs_con(userName, passWord):\n",
    "    \"\"\"\n",
    "     module name  : c_setup_dbs_con\n",
    "     purpose      : Setup database connection\n",
    "     parameter    : \n",
    "       userName: User name to access Teradata \n",
    "       passWord: Password to access Teradata \n",
    "     Return       :  \n",
    "       session    : udaExec  connection\n",
    "       td_enginex : Teradata engine \n",
    "    Notes:\n",
    "     - Need to import the following packages/libraries \n",
    "       import sqlalchemy\n",
    "       from sqlalchemy import create_engine\n",
    "       import teradata     \n",
    "    \"\"\"\n",
    "    udaExec = teradata.UdaExec (appName=\"Teradata_Test\", version=\"1.0\", logConsole=False)\n",
    "    session = udaExec.connect(method=\"odbc\", system=\"tqdpr02\",\n",
    "            username = userName, password= passWord )\n",
    "    t_engine   = 'teradata://{0}:{1}@tqdpr02/temp_tables'.format(userName, passWord)\n",
    "    print (\"t_engine\", t_engine)\n",
    "    td_enginex = create_engine(t_engine) \n",
    "    return session,  td_enginex "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cf_ld_topN_nbrK_tbl(df_cmb, tbl_cmb,  cr_tbl_ind, td_enginex ):\n",
    "    \"\"\"\n",
    "     Module name  : c_cr_topN_nbrK_tbl\n",
    "     Purpose      : Load data from the dataframe df_cmb which had cimbined recommendted items and K neighbors to a table\n",
    "     Parameter    : \n",
    "       df_cmb     : Dataframe which had cimbined recommendted items and K neighbors\n",
    "       tbl_cmb    : Table name to load the dataframe df_cmb \n",
    "                    - Schema name is not required\n",
    "                    - Schema name will define in td_enginex\n",
    "       cr_tbl_ind :\n",
    "         True     : will creat a new table\n",
    "         False    : Will not creat a new tabl\n",
    "       td_enginex : Connection of Teradata engine   \n",
    "     Return       : N/A \n",
    "            \n",
    "    \"\"\"\n",
    "    fnc_name = inspect.stack()[0][3]\n",
    "    start_time = time.time()\n",
    "    \"\"\"\n",
    "     - _df_cmb is the copy of df_cmb\n",
    "     - The change of _df_cmb wii not have impact on db_cmb\n",
    "    \"\"\"\n",
    "    _df_cmb = df_cmb.copy() # is different from  _df_cmb = df_cmb\n",
    "    _df_cmb.index.names=['custno'] # Setup dummy index\n",
    "   \n",
    "#    if cr_tbl_ind == True:\n",
    "#       try: \n",
    "#          _df_cmb.to_sql(con=td_enginex, name=tbl_cmb, if_exists='replace')  \n",
    "#       DatabaseError:     \n",
    "#           \n",
    "    _df_cmb.to_sql(con=td_enginex, name=tbl_cmb, if_exists='replace', index = False)\n",
    "    end_time = time.time()\n",
    "    cf_elapse_time (  start_time, end_time, \"Function {0} to load table {1} completed.\".format(fnc_name, tbl_cmb))     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_engine teradata://syue003:Chungli#1@tqdpr02/temp_tables\n",
      "tbl_cmb rs_s0025_junaug19_knnb_cosine_meannrm\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    " Step0: Initialization \n",
    "  - Import packages/libraries from the file c_import.py\n",
    "  - Setup database connection \n",
    "\"\"\"\n",
    "prg_name = \"\"\n",
    "path_code = \"C:\\\\Users\\\\syue003\\\\wip_RecSys\\\\\"\n",
    "c_import  = path_code + \"c_import.py\"\n",
    "c_hitrate = path_code + \"c_hit_rate.py\" \n",
    "c_timedte = path_code + \"c_time_dte.py\" \n",
    "\n",
    "exec(compile(open(c_import, 'rb').read(), c_import,  'exec'))\n",
    "exec(compile(open(c_hitrate, 'rb').read(),c_hitrate, 'exec'))\n",
    "exec(compile(open(c_timedte, 'rb').read(),c_timedte, 'exec'))\n",
    "session, td_enginex = cf_setup_dbs_con(userName = 'syue003', passWord = 'Chungli#1')\n",
    "\n",
    "\"\"\"\n",
    " Define table name  \n",
    "\"\"\"\n",
    "\n",
    "str_id = '0025_'             # store id\n",
    "duration = 'junaug19_'\n",
    "mean_nrm = '_meannrm'        # Indicator if applies mean for the normization \n",
    "knn_algm = KNNBasic          # KNN algorithm to calculate Similarity Matrix  \n",
    "knn_algm_abb = 'knnb_'       # Abbrevation of KNN algorithm\n",
    "sim_mea = 'cosine'           # Similarity measure\n",
    "tbl_cmb = \"rs_s\" + str_id + duration + knn_algm_abb + sim_mea + mean_nrm \n",
    "print ( \"tbl_cmb\", tbl_cmb )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tnx_tbl temp_tables.rs_pos_tnx_s0025_junaug19_sum N =  15306 m = 3972 10 10\n",
      "session <teradata.udaexec.UdaExecConnection object at 0x0000000008F29198>\n",
      "The total no of users in ft = data.build_full_trainset(): 15266 The total no of items in ft: 3972\n",
      "The total number of users in trainSet: 15151 The total number of items in trainSet: 3972\n",
      "The total length of testSet: 15266 \n",
      "Example of testSet: [(76782197.0, 1252300.0, 1.0), (58669755.0, 1491527.0, 1.0)]\n"
     ]
    }
   ],
   "source": [
    "#4.1\n",
    "\"\"\"\n",
    " Step1: Invoke cf_build_trainset \n",
    "   - Create surprise trainSet and testSet\n",
    "\"\"\"\n",
    "N= 15306; M = 3972; maxScale = 10\n",
    "tnx_tbl = \"temp_tables.rs_pos_tnx_s0025_junaug19_sum\"\n",
    "trainSet, testSet = cf_build_trainset(tnx_tbl, N , M , maxScale, session = session )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c_get_qty_mean - df.head    iprod_sk  prod_sk  unit_qty\n",
      "0         0   618044       1.0\n",
      "1         1  1145662       5.0\n",
      "2         2  2434543       1.0\n",
      "3         3    20543       1.0\n",
      "4         4   659732       2.0\n",
      " Function cf_get_unit_qty_mean completed. It took 1.627000 seconds - 0hh:0mm:1ss.\n",
      " start time: Sep 23 2019 15:16:16  end time:  Sep 23 2019 15:16:17\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "  Step2: Invoke cf_get_unit_qty_mean\n",
    "   - Create a dictionay with key as iprod_sk and value as mean of unit_qty from trainSet                   \n",
    "   - Create a dataframe with columns of iprod_sk, prod_sk, and unit_qty_mean   \n",
    "\"\"\"\n",
    "if mean_nrm != \"\":\n",
    "   d_mean, df_mean = cf_get_unit_qty_mean(trainSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Function cf_get_simsMatrix completed. It took 152.063993 seconds - 0hh:2mm:32ss.\n",
      " start time: Sep 23 2019 15:16:19  end time:  Sep 23 2019 15:18:51\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "  Step3: Invoke cf_get_simsMatrix to get simularity matrix \n",
    "\"\"\"    \n",
    "simsMatrix, model = cf_get_simsMatrix(trainSet, sim_measure = sim_mea , user_base_ind = True, knn_algm = knn_algm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Function cf_get_userbase_topN completed. It took 163.179993 seconds - 0hh:2mm:43ss.\n",
      " start time: Sep 23 2019 15:18:51  end time:  Sep 23 2019 15:21:34\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "  Step4: Invoke cf_get_userbase_topN\n",
    "   - Get top 10 recommendation\n",
    "\"\"\"\n",
    "K = 10\n",
    "N = 10\n",
    "topN, nbrK_ary = cf_get_userbase_topN(trainSet, simsMatrix, K, N, d_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HR 0.021551159439276824 for top  10 top  10 neighbors\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "  Step5: Invoke cf_HitRate to get hit rate \n",
    "   - Get top 10 recommendation\n",
    "  Notes:\n",
    "   - without applying unit_qty_mean for the normalization,  the HR =  0.024957421721472552 \n",
    "   - Applying unit_qty_mean for the normalization,  the HR = 0  \n",
    "   - leftOutPredictions is a \"prediction\" object containing:\n",
    "     The (raw) user id uid.\n",
    "     The (raw) item id iid.\n",
    "     The true rating\n",
    "     The estimated rating   \n",
    "\"\"\"\n",
    "topN_x = topN.copy() # backup topN\n",
    "leftOutPredictions = model.test(testSet)   \n",
    "print(\"HR\", round(cf_HitRate(topN, leftOutPredictions), 4), \"for top \", K, \"top \", N, \"neighbors\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[76782197, 622127, 3.3598, 1068396, 3.2342, 1491527, 2.9381, 632059, 2.7929, 82267, 2.7906, 1290148, 2.4831, 1127635, 2.451, 592484, 2.449, 625466, 2.2783, 483819, 2.2534]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "  Step6: Invoke cf_comb_topN_nbrK  \n",
    "   - Merge top N recommendted items and top K neighbors\n",
    "     -  knbr: k nearest raw user id\n",
    "     -  scrs: simarity score\n",
    "\"\"\"\n",
    "#Merge recommendted items and similar user\n",
    "lbl_topN = ['hh_sk'] + ['prod_1','scr_1','prod_2','scr_2','prod_3','scr_3','prod_4','scr_4','prod_5','scr_5',\n",
    "          'prod_6','scr_6','prod_7','scr_7', 'prod_8','scr_8','prod_9','scr_9','prod_10','scr_10']\n",
    "lbl_nbrK= ['hh_sk'] + ['knbr_1','scrs_1','knbr_2','scrs_2','knbr_3','scrs_3','knbr_4','scrs_4','knbr_5','scrs_5',\n",
    "          'knbr_6','scrs_6','knbr_7','scrs_7', 'knbr_8','scrs_8','knbr_9','scrs_9','knbr_10','scrs_10']\n",
    "\n",
    "df_topN_nbrK = cf_comb_topN_nbrK(topN, nbrK_ary, lbl_topN, lbl_nbrK )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Function cf_ld_topN_nbrK_tbl to load table rs_s0025_junaug19_knnb_cosine_meannrm completed. It took 410.344988 seconds - 0hh:6mm:50ss.\n",
      " start time: Sep 23 2019 15:21:41  end time:  Sep 23 2019 15:28:31\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    " step7: Load the combined dataframe to a table \n",
    "\"\"\"\n",
    "cr_tbl_ind = True\n",
    "cf_ld_topN_nbrK_tbl(df_topN_nbrK, tbl_cmb,  cr_tbl_ind, td_enginex = td_enginex )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AB test\n",
    "def f_rs_comp(trainSet, testSet, knn_algm, sim_mea, nrm_adj, K = 10, N = 10):\n",
    "    \n",
    "   start_time = time.time() \n",
    "   \"\"\"\n",
    "     Invoke cf_get_simsMatrix to get simularity matrix \n",
    "   \"\"\"    \n",
    "   simsMatrix, model = cf_get_simsMatrix(trainSet, sim_measure = sim_mea , user_base_ind = True, knn_algm = knn_algm)\n",
    "   \"\"\"\n",
    "     Invoke cf_get_userbase_topN\n",
    "     - Get top 10 recommendation\n",
    "   \"\"\"\n",
    "   topN, nbrK_ary = cf_get_userbase_topN(trainSet, simsMatrix, K, N, nrm_adj)\n",
    "  \n",
    "   topN_x = topN.copy() # backup topN\n",
    "   leftOutPredictions = model.test(testSet)   \n",
    "   hr = round(cf_HitRate(topN, leftOutPredictions), 4)\n",
    "   print(\"Hit Rate = {0} with algorithm = {1}, Similarity Measure = {2}\".format(hr, knn_algm, sim_mea ))\n",
    "   if  type(nrm_adj) is dict:        \n",
    "       print(\"for top {0} recommended products and {1} nearest neighbors applied product mean for normalization\".format(K, N) )\n",
    "   else:    \n",
    "       print(\"for top {1} recommended products and {1} nearest neighbors applied max unit_qty = {2} for the normalization\".format(K, N, nrm_adj) )\n",
    "  \n",
    "   end_time = time.time()\n",
    "   cf_elapse_time (start_time, end_time, dsc = ' ')\n",
    "   return topN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Function cf_get_simsMatrix completed. It took 161.642000 seconds - 0hh:2mm:41ss.\n",
      " start time: Sep 24 2019 10:38:01  end time:  Sep 24 2019 10:40:42\n",
      " Function cf_get_userbase_topN completed. It took 217.798000 seconds - 0hh:3mm:37ss.\n",
      " start time: Sep 24 2019 10:40:42  end time:  Sep 24 2019 10:44:20\n",
      "Hit Rate = 0.0216 with algorithm = <class 'surprise.prediction_algorithms.knns.KNNBasic'>, Similarity Measure = cosine\n",
      "for top 10 recommended products and 10 nearest neighbors applied product mean for normalization\n",
      "   It took 385.076000 seconds - 0hh:6mm:25ss.\n",
      " start time: Sep 24 2019 10:38:00  end time:  Sep 24 2019 10:44:26\n"
     ]
    }
   ],
   "source": [
    "f_rs_comp(trainSet, testSet, KNNBasic, 'cosine', d_mean, K = 10, N = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Function cf_get_simsMatrix completed. It took 135.784000 seconds - 0hh:2mm:15ss.\n",
      " start time: Sep 24 2019 11:03:45  end time:  Sep 24 2019 11:06:01\n",
      " Function cf_get_userbase_topN completed. It took 153.710000 seconds - 0hh:2mm:33ss.\n",
      " start time: Sep 24 2019 11:06:01  end time:  Sep 24 2019 11:08:35\n",
      "Hit Rate = 0.0216 with algorithm = <class 'surprise.prediction_algorithms.knns.KNNBasic'>, Similarity Measure = cosine\n",
      "for top 10 recommended products and 10 nearest neighbors applied product mean for normalization\n",
      "   It took 294.910000 seconds - 0hh:4mm:54ss.\n",
      " start time: Sep 24 2019 11:03:45  end time:  Sep 24 2019 11:08:40\n"
     ]
    }
   ],
   "source": [
    "topN_KB_coc_mean = f_rs_comp(trainSet, testSet, KNNBasic, 'cosine', d_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Function cf_get_simsMatrix completed. It took 138.287000 seconds - 0hh:2mm:18ss.\n",
      " start time: Sep 24 2019 11:24:56  end time:  Sep 24 2019 11:27:14\n",
      " Function cf_get_userbase_topN completed. It took 153.251000 seconds - 0hh:2mm:33ss.\n",
      " start time: Sep 24 2019 11:27:14  end time:  Sep 24 2019 11:29:48\n",
      "Hit Rate = 0.0216 with algorithm = <class 'surprise.prediction_algorithms.knns.KNNWithZScore'>, Similarity Measure = cosine\n",
      "for top 10 recommended products and 10 nearest neighbors applied product mean for normalization\n",
      "   It took 297.455000 seconds - 0hh:4mm:57ss.\n",
      " start time: Sep 24 2019 11:24:56  end time:  Sep 24 2019 11:29:53\n",
      " Function cf_get_simsMatrix completed. It took 132.679000 seconds - 0hh:2mm:12ss.\n",
      " start time: Sep 24 2019 11:29:54  end time:  Sep 24 2019 11:32:07\n",
      " Function cf_get_userbase_topN completed. It took 159.531000 seconds - 0hh:2mm:39ss.\n",
      " start time: Sep 24 2019 11:32:07  end time:  Sep 24 2019 11:34:46\n",
      "Hit Rate = 0.0216 with algorithm = <class 'surprise.prediction_algorithms.knns.KNNWithMeans'>, Similarity Measure = cosine\n",
      "for top 10 recommended products and 10 nearest neighbors applied product mean for normalization\n",
      "   It took 297.941000 seconds - 0hh:4mm:57ss.\n",
      " start time: Sep 24 2019 11:29:54  end time:  Sep 24 2019 11:34:52\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'KNNBasicline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-ea3feac0cfdb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtopN_KZ_cos_mean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf_rs_comp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainSet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestSet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mKNNWithZScore\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'cosine'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md_mean\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtopN_KM_cos_mean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf_rs_comp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainSet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestSet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mKNNWithMeans\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'cosine'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md_mean\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtopN_KBL_cos_mean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf_rs_comp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainSet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestSet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mKNNBasicline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'cosine'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md_mean\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'KNNBasicline' is not defined"
     ]
    }
   ],
   "source": [
    "topN_KZ_cos_mean = f_rs_comp(trainSet, testSet, KNNWithZScore, 'cosine', d_mean)\n",
    "topN_KM_cos_mean = f_rs_comp(trainSet, testSet, KNNWithMeans, 'cosine', d_mean)\n",
    "topN_KBL_cos_mean = f_rs_comp(trainSet, testSet, KNNBaseline, 'cosine', d_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get most p[opular hh_sk  and prod_sk\n",
    "user_ids_count = Counter(df_sum_qty.HH_SK)   # type: collections.Counter\n",
    "prod_ids_count = Counter(df_sum_qty.PROD_SK)\n",
    "n = 15306\n",
    "m = 3972\n",
    "user_ids = [u for u, c in user_ids_count.most_common(n)]\n",
    "prod_ids = [m for m, c in prod_ids_count.most_common(m)]\n",
    "df_sum_qty_final = df_sum_qty[df_sum_qty.HH_SK.isin(user_ids) & df_sum_qty.PROD_SK.isin(prod_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Function cf_get_simsMatrix completed. It took 137.354000 seconds - 0hh:2mm:17ss.\n",
      " start time: Sep 24 2019 11:41:26  end time:  Sep 24 2019 11:43:44\n",
      " Function cf_get_userbase_topN completed. It took 153.775000 seconds - 0hh:2mm:33ss.\n",
      " start time: Sep 24 2019 11:43:44  end time:  Sep 24 2019 11:46:18\n",
      "Hit Rate = 0.0216 with algorithm = <class 'surprise.prediction_algorithms.knns.KNNBaseline'>, Similarity Measure = cosine\n",
      "for top 10 recommended products and 10 nearest neighbors applied product mean for normalization\n",
      "   It took 297.064000 seconds - 0hh:4mm:57ss.\n",
      " start time: Sep 24 2019 11:41:26  end time:  Sep 24 2019 11:46:23\n"
     ]
    }
   ],
   "source": [
    "topN_KBL_cos_mean = f_rs_comp(trainSet, testSet, KNNBaseline, 'cosine', d_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Function cf_get_simsMatrix completed. It took 147.971000 seconds - 0hh:2mm:27ss.\n",
      " start time: Sep 24 2019 12:01:04  end time:  Sep 24 2019 12:03:32\n",
      " Function cf_get_userbase_topN completed. It took 153.381000 seconds - 0hh:2mm:33ss.\n",
      " start time: Sep 24 2019 12:03:32  end time:  Sep 24 2019 12:06:06\n",
      "Hit Rate = 0.025 with algorithm = <class 'surprise.prediction_algorithms.knns.KNNBasic'>, Similarity Measure = cosine\n",
      "for top 10 recommended products and 10 nearest neighbors applied max unit_qty = 10 for the normalization\n",
      "   It took 307.521000 seconds - 0hh:5mm:7ss.\n",
      " start time: Sep 24 2019 12:01:04  end time:  Sep 24 2019 12:06:12\n"
     ]
    }
   ],
   "source": [
    "topN_KB_cos_maxr = f_rs_comp(trainSet, testSet, KNNBasic, 'cosine', nrm_adj = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Function cf_get_simsMatrix completed. It took 137.634000 seconds - 0hh:2mm:17ss.\n",
      " start time: Sep 24 2019 12:17:07  end time:  Sep 24 2019 12:19:25\n",
      " Function cf_get_userbase_topN completed. It took 153.988000 seconds - 0hh:2mm:33ss.\n",
      " start time: Sep 24 2019 12:19:25  end time:  Sep 24 2019 12:21:59\n",
      "Hit Rate = 0.025 with algorithm = <class 'surprise.prediction_algorithms.knns.KNNWithZScore'>, Similarity Measure = cosine\n",
      "for top 10 recommended products and 10 nearest neighbors applied max unit_qty = 10 for the normalization\n",
      "   It took 297.304000 seconds - 0hh:4mm:57ss.\n",
      " start time: Sep 24 2019 12:17:07  end time:  Sep 24 2019 12:22:04\n",
      " Function cf_get_simsMatrix completed. It took 136.124000 seconds - 0hh:2mm:16ss.\n",
      " start time: Sep 24 2019 12:22:05  end time:  Sep 24 2019 12:24:21\n",
      " Function cf_get_userbase_topN completed. It took 153.678000 seconds - 0hh:2mm:33ss.\n",
      " start time: Sep 24 2019 12:24:21  end time:  Sep 24 2019 12:26:55\n",
      "Hit Rate = 0.025 with algorithm = <class 'surprise.prediction_algorithms.knns.KNNWithMeans'>, Similarity Measure = cosine\n",
      "for top 10 recommended products and 10 nearest neighbors applied max unit_qty = 10 for the normalization\n",
      "   It took 295.288000 seconds - 0hh:4mm:55ss.\n",
      " start time: Sep 24 2019 12:22:05  end time:  Sep 24 2019 12:27:00\n",
      " Function cf_get_simsMatrix completed. It took 138.997000 seconds - 0hh:2mm:18ss.\n",
      " start time: Sep 24 2019 12:27:01  end time:  Sep 24 2019 12:29:20\n",
      " Function cf_get_userbase_topN completed. It took 159.946000 seconds - 0hh:2mm:39ss.\n",
      " start time: Sep 24 2019 12:29:20  end time:  Sep 24 2019 12:32:00\n",
      "Hit Rate = 0.025 with algorithm = <class 'surprise.prediction_algorithms.knns.KNNBaseline'>, Similarity Measure = cosine\n",
      "for top 10 recommended products and 10 nearest neighbors applied max unit_qty = 10 for the normalization\n",
      "   It took 305.457000 seconds - 0hh:5mm:5ss.\n",
      " start time: Sep 24 2019 12:27:01  end time:  Sep 24 2019 12:32:06\n"
     ]
    }
   ],
   "source": [
    "topN_KZ_cos_maxr = f_rs_comp(trainSet, testSet, KNNWithZScore, 'cosine', nrm_adj = 10)\n",
    "topN_KM_cos_maxr = f_rs_comp(trainSet, testSet, KNNWithMeans, 'cosine',  nrm_adj = 10)\n",
    "topN_KBL_cos_maxr = f_rs_comp(trainSet, testSet, KNNBaseline, 'cosine',  nrm_adj = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Function cf_get_simsMatrix completed. It took 124.425000 seconds - 0hh:2mm:4ss.\n",
      " start time: Sep 24 2019 12:34:23  end time:  Sep 24 2019 12:36:27\n",
      " Function cf_get_userbase_topN completed. It took 174.636000 seconds - 0hh:2mm:54ss.\n",
      " start time: Sep 24 2019 12:36:27  end time:  Sep 24 2019 12:39:22\n",
      "Hit Rate = 0.0434 with algorithm = <class 'surprise.prediction_algorithms.knns.KNNBasic'>, Similarity Measure = pearson_baseline\n",
      "for top 10 recommended products and 10 nearest neighbors applied max unit_qty = 10 for the normalization\n",
      "   It took 304.254000 seconds - 0hh:5mm:4ss.\n",
      " start time: Sep 24 2019 12:34:23  end time:  Sep 24 2019 12:39:27\n",
      " Function cf_get_simsMatrix completed. It took 117.510000 seconds - 0hh:1mm:57ss.\n",
      " start time: Sep 24 2019 12:39:27  end time:  Sep 24 2019 12:41:25\n",
      " Function cf_get_userbase_topN completed. It took 181.138000 seconds - 0hh:3mm:1ss.\n",
      " start time: Sep 24 2019 12:41:25  end time:  Sep 24 2019 12:44:26\n",
      "Hit Rate = 0.0434 with algorithm = <class 'surprise.prediction_algorithms.knns.KNNBaseline'>, Similarity Measure = pearson_baseline\n",
      "for top 10 recommended products and 10 nearest neighbors applied max unit_qty = 10 for the normalization\n",
      "   It took 304.566000 seconds - 0hh:5mm:4ss.\n",
      " start time: Sep 24 2019 12:39:27  end time:  Sep 24 2019 12:44:32\n",
      " Function cf_get_simsMatrix completed. It took 120.216000 seconds - 0hh:2mm:0ss.\n",
      " start time: Sep 24 2019 12:44:33  end time:  Sep 24 2019 12:46:33\n",
      " Function cf_get_userbase_topN completed. It took 182.247000 seconds - 0hh:3mm:2ss.\n",
      " start time: Sep 24 2019 12:46:33  end time:  Sep 24 2019 12:49:35\n",
      "Hit Rate = 0.0434 with algorithm = <class 'surprise.prediction_algorithms.knns.KNNWithZScore'>, Similarity Measure = pearson_baseline\n",
      "for top 10 recommended products and 10 nearest neighbors applied max unit_qty = 10 for the normalization\n",
      "   It took 308.226000 seconds - 0hh:5mm:8ss.\n",
      " start time: Sep 24 2019 12:44:32  end time:  Sep 24 2019 12:49:41\n",
      " Function cf_get_simsMatrix completed. It took 119.588000 seconds - 0hh:1mm:59ss.\n",
      " start time: Sep 24 2019 12:49:41  end time:  Sep 24 2019 12:51:41\n",
      " Function cf_get_userbase_topN completed. It took 176.766000 seconds - 0hh:2mm:56ss.\n",
      " start time: Sep 24 2019 12:51:41  end time:  Sep 24 2019 12:54:38\n",
      "Hit Rate = 0.0434 with algorithm = <class 'surprise.prediction_algorithms.knns.KNNWithMeans'>, Similarity Measure = pearson_baseline\n",
      "for top 10 recommended products and 10 nearest neighbors applied max unit_qty = 10 for the normalization\n",
      "   It took 301.811000 seconds - 0hh:5mm:1ss.\n",
      " start time: Sep 24 2019 12:49:41  end time:  Sep 24 2019 12:54:43\n"
     ]
    }
   ],
   "source": [
    "topN_KB_peab_maxr = f_rs_comp(trainSet, testSet, KNNBasic, 'pearson_baseline',  nrm_adj = 10)\n",
    "topN_KBL_peab_maxr = f_rs_comp(trainSet, testSet, KNNBaseline, 'pearson_baseline',  nrm_adj = 10)\n",
    "topN_KZ_peab_maxr = f_rs_comp(trainSet, testSet, KNNWithZScore, 'pearson_baseline', nrm_adj = 10)\n",
    "topN_KM_peab_maxr = f_rs_comp(trainSet, testSet, KNNWithMeans, 'pearson_baseline',  nrm_adj = 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Function cf_get_simsMatrix completed. It took 119.278000 seconds - 0hh:1mm:59ss.\n",
      " start time: Sep 24 2019 12:54:44  end time:  Sep 24 2019 12:56:43\n",
      " Function cf_get_userbase_topN completed. It took 180.052000 seconds - 0hh:3mm:0ss.\n",
      " start time: Sep 24 2019 12:56:43  end time:  Sep 24 2019 12:59:43\n",
      "Hit Rate = 0.0426 with algorithm = <class 'surprise.prediction_algorithms.knns.KNNBasic'>, Similarity Measure = pearson_baseline\n",
      "for top 10 recommended products and 10 nearest neighbors applied product mean for normalization\n",
      "   It took 304.545000 seconds - 0hh:5mm:4ss.\n",
      " start time: Sep 24 2019 12:54:44  end time:  Sep 24 2019 12:59:48\n",
      " Function cf_get_simsMatrix completed. It took 117.979000 seconds - 0hh:1mm:57ss.\n",
      " start time: Sep 24 2019 12:59:49  end time:  Sep 24 2019 13:01:47\n",
      " Function cf_get_userbase_topN completed. It took 186.788000 seconds - 0hh:3mm:6ss.\n",
      " start time: Sep 24 2019 13:01:47  end time:  Sep 24 2019 13:04:54\n",
      "Hit Rate = 0.0426 with algorithm = <class 'surprise.prediction_algorithms.knns.KNNBaseline'>, Similarity Measure = pearson_baseline\n",
      "for top 10 recommended products and 10 nearest neighbors applied product mean for normalization\n",
      "   It took 310.716000 seconds - 0hh:5mm:10ss.\n",
      " start time: Sep 24 2019 12:59:49  end time:  Sep 24 2019 13:04:59\n",
      " Function cf_get_simsMatrix completed. It took 124.890000 seconds - 0hh:2mm:4ss.\n",
      " start time: Sep 24 2019 13:05:00  end time:  Sep 24 2019 13:07:05\n",
      " Function cf_get_userbase_topN completed. It took 188.397000 seconds - 0hh:3mm:8ss.\n",
      " start time: Sep 24 2019 13:07:05  end time:  Sep 24 2019 13:10:13\n",
      "Hit Rate = 0.0426 with algorithm = <class 'surprise.prediction_algorithms.knns.KNNWithZScore'>, Similarity Measure = pearson_baseline\n",
      "for top 10 recommended products and 10 nearest neighbors applied product mean for normalization\n",
      "   It took 318.854000 seconds - 0hh:5mm:18ss.\n",
      " start time: Sep 24 2019 13:05:00  end time:  Sep 24 2019 13:10:19\n",
      " Function cf_get_simsMatrix completed. It took 119.305000 seconds - 0hh:1mm:59ss.\n",
      " start time: Sep 24 2019 13:10:19  end time:  Sep 24 2019 13:12:19\n",
      " Function cf_get_userbase_topN completed. It took 184.754000 seconds - 0hh:3mm:4ss.\n",
      " start time: Sep 24 2019 13:12:19  end time:  Sep 24 2019 13:15:23\n",
      "Hit Rate = 0.0426 with algorithm = <class 'surprise.prediction_algorithms.knns.KNNWithMeans'>, Similarity Measure = pearson_baseline\n",
      "for top 10 recommended products and 10 nearest neighbors applied product mean for normalization\n",
      "   It took 309.866000 seconds - 0hh:5mm:9ss.\n",
      " start time: Sep 24 2019 13:10:19  end time:  Sep 24 2019 13:15:29\n"
     ]
    }
   ],
   "source": [
    "topN_KB_peab_mean = f_rs_comp(trainSet, testSet, KNNBasic, 'pearson_baseline',  nrm_adj = d_mean)\n",
    "topN_KBL_peab_mean = f_rs_comp(trainSet, testSet, KNNBaseline, 'pearson_baseline',  nrm_adj = d_mean)\n",
    "topN_KZ_peab_mean = f_rs_comp(trainSet, testSet, KNNWithZScore, 'pearson_baseline', nrm_adj = d_mean)\n",
    "topN_KM_peab_mean = f_rs_comp(trainSet, testSet, KNNWithMeans, 'pearson_baseline',  nrm_adj = d_mean)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
